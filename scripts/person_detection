#!/usr/bin/env python

import sys
import rospy
import cv2
from std_msgs.msg import String
from sensor_msgs.msg import Image
from cv_bridge import CvBridge, CvBridgeError
import PIL.Image
import numpy
import numpy as np
import sensor_msgs
import std_msgs
import person_detection
import person_detection.msg
import sensor_msgs.msg
import tf
import visualization_msgs.msg
import compressed_depth_image_transport
import geometry_msgs.msg

rospy.init_node("person_detection", anonymous=True)

tf_listener = tf.TransformListener()

#db = "haarcascade_frontalface_default.xml"
#db = "haarcascade_frontalface_alt.xml"
#db = "haarcascade_frontalcatface_extended.xml"
#db = "haarcascade_fullbody.xml"
#db = "haarcascade_frontalface_alt2.xml"
db = rospy.get_param("~face_detector_model")
face_cascade = cv2.CascadeClassifier(db)

bridge = CvBridge()

pub_image_visualization = rospy.Publisher("~visualization", sensor_msgs.msg.Image, queue_size=1)
pub_image_clusters = rospy.Publisher("~clusters", sensor_msgs.msg.Image, queue_size=1)
pub_image_points = rospy.Publisher("~points", sensor_msgs.msg.Image, queue_size=1)

face_marker = rospy.get_param("~face_marker")
#print("b", face_marker)
face_marker = cv2.imread(face_marker, cv2.IMREAD_UNCHANGED)

face_marker[:,:,3] -= face_marker[:,:,3] / 3

depth = False
depth_image_header = False
def depth_callback(data):
    global bridge, depth, depth_image_header
    try:
        image = bridge.imgmsg_to_cv2(data)
    except CvBridgeError as e:
        print(e)
        return
    depth = image
    depth_image_header = data.header

color_image = False
def image_callback(data):
    global bridge, face_cascade, depth, color_image
    try:
        image = bridge.compressed_imgmsg_to_cv2(data, "bgra8")
    except CvBridgeError as e:
        print(e)
        return
    color_image = image

camera_depth_info = False
def on_camera_depth_info(msg):
    global camera_depth_info
    #print("bla")
    camera_depth_info = msg

enabled = False
sub_camera_depth_info = False
image_sub = False
depth_sub = False
def enable(x):
    global enabled, sub_camera_depth_info, image_sub, depth_sub
    if enabled: return
    print("enable")
    sub_camera_depth_info = rospy.Subscriber("camera_depth_info", sensor_msgs.msg.CameraInfo, on_camera_depth_info, queue_size=1)
    image_sub = rospy.Subscriber("camera_rgb", sensor_msgs.msg.CompressedImage, image_callback, queue_size=1)
    depth_sub = rospy.Subscriber("camera_depth", sensor_msgs.msg.Image, depth_callback, queue_size=1)
    enabled = True
def disable(x):
    global enabled, sub_camera_depth_info, image_sub, depth_sub
    if not enabled: return
    print("disable")
    sub_camera_depth_info.unregister()
    image_sub.unregister()
    depth_sub.unregister()
    sub_camera_depth_info = False
    image_sub = False
    depth_sub = False
    enabled = False
def set_enabled(x):
    global enabled
    x = x.data
    if not enabled and x:
        enable(0)
    if enabled and not x:
        disable(0)
if rospy.get_param("~enabled", False):
    enable(0)
sub_enable = rospy.Subscriber("~enable", std_msgs.msg.Empty, enable, queue_size=1)
sub_disable = rospy.Subscriber("~disable", std_msgs.msg.Empty, disable, queue_size=1)
sub_enabled = rospy.Subscriber("~set_enabled", std_msgs.msg.Bool, set_enabled, queue_size=1)

camera_depth_frame = rospy.get_param("~camera_depth_frame")

pub_person_detections = rospy.Publisher("~person_detections", person_detection.msg.PersonDetections, queue_size=1)

pub_markers = rospy.Publisher("~person_markers", visualization_msgs.msg.MarkerArray, queue_size=5, latch=True)



def floodFill2(image, mask, point, value):
    if image[point[1], point[0]] > 0:
        cv2.floodFill(image, mask, point, value)

last_faces = False

zone_to_world_translation = (
    rospy.get_param("/tiago_bartender/customer_zone/center_x"),
    rospy.get_param("/tiago_bartender/customer_zone/center_y"),
    rospy.get_param("/tiago_bartender/customer_zone/center_z"),
    )
zone_to_world_rotation = (0, 0, rospy.get_param("/tiago_bartender/customer_zone/euler_z"))
zone_to_world_scaling = (
    rospy.get_param("/tiago_bartender/customer_zone/size_x"),
    rospy.get_param("/tiago_bartender/customer_zone/size_y"),
    rospy.get_param("/tiago_bartender/customer_zone/size_z"),
    )
zone_to_world = tf.transformations.compose_matrix(
    scale=zone_to_world_scaling,
    angles=zone_to_world_rotation,
    translate=zone_to_world_translation,
)

def update():

    global bridge, face_cascade, depth, color_image, last_faces

    if camera_depth_info is False:
        return

    if depth_image_header is False:
        return



    K = np.array(camera_depth_info.K, dtype=float)
    K = np.reshape(K, (3,3,))

    view_to_px = np.reshape(camera_depth_info.K, (3,3,))
    px_to_view = np.linalg.inv(view_to_px)

    image = color_image
    if image is False:
        print "no image"
        return
    image = image.copy()

    if False:
        image = cv2.resize(image, (0,0), fx=1.1, fy=1.1)

    y = (image.shape[1] - color_image.shape[1]) / 2
    x = (image.shape[0] - color_image.shape[0]) / 2
    w = color_image.shape[1]
    h = color_image.shape[0]
    image = image[y:y+h,x:x+w,:]

    if depth is False:
        print "no depth"
        return

    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    faces = face_cascade.detectMultiScale(gray, 1.2, 1)

    if last_faces is not False and len(last_faces) > 0:
      for face in faces:
        best_match = last_faces[0]
        for face_prev in last_faces:
          if best_match is not False and np.linalg.norm(np.array(best_match)[0:2] - np.array(face)[0:2]) > np.linalg.norm(np.array(face_prev)[0:2] - np.array(face)[0:2]):
            best_match = face_prev
        if False:
            if best_match is not False:
              face[2] = face[2] * 0.2 + best_match[2] * 0.8
              face[3] = face[3] * 0.2 + best_match[3] * 0.8

    last_faces = faces

    d = depth.copy()

    d[:,:] = 1024
    m = np.logical_and(depth > 0.0, np.isfinite(depth))
    d[m] = depth[m]

    #d = cv2.medianBlur(d, 7)
    #d = cv2.erode(d, np.ones((2,2),np.uint8), iterations = 15)

    d[d == 1024] = 0

    #dx = cv2.Sobel(d, cv2.CV_32F, 1, 0, ksize = 7)
    #dy = cv2.Sobel(d, cv2.CV_32F, 0, 1, ksize = 7)
    #edges = cv2.max(cv2.max(dx, -dx), cv2.max(dy, -dy))
    #edges = edges * 0.03

    #ee = cv2.medianBlur(edges, 7)
    #dx = cv2.Sobel(ee, cv2.CV_32F, 1, 0, ksize = 7)
    #dy = cv2.Sobel(ee, cv2.CV_32F, 0, 1, ksize = 7)
    #edges = cv2.max(cv2.max(dx, -dx), cv2.max(dy, -dy)) * 0.015

    d = d * (1.0 / 1000)

    depth_image = d

    points = np.zeros((depth_image.shape[0], depth_image.shape[1], 3))
    points[:,:,0] = np.multiply(depth_image, np.repeat([np.arange(0, depth_image.shape[1])], depth_image.shape[0], 0) * px_to_view[0, 0] + px_to_view[0, 2])
    points[:,:,1] = np.multiply(depth_image, np.reshape(np.repeat(np.arange(0, depth_image.shape[0]), depth_image.shape[1], 0), depth_image.shape) * px_to_view[1, 1] + px_to_view[1, 2])
    points[:,:,2] = depth_image

    #view_to_world = tf_listener.lookupTransform("map", depth_image_header.frame_id, depth_image_header.stamp)
    view_to_world = tf_listener.lookupTransform(rospy.get_param("/tiago_bartender/customer_zone/frame"), depth_image_header.frame_id, tf_listener.getLatestCommonTime(rospy.get_param("/tiago_bartender/customer_zone/frame"), depth_image_header.frame_id))
    #print view_to_world

    '''
    view_to_world_rotation = tf.transformations.quaternion_matrix(view_to_world[1])
    view_to_world_translation = view_to_world[0]

    pp = points.copy()
    for i in range(3):
        points[:,:,i] = pp[:,:,0]*view_to_world_rotation[i,0] + pp[:,:,1]*view_to_world_rotation[i,1] + pp[:,:,2]*view_to_world_rotation[i,2] + view_to_world_translation[i]
    '''

    view_to_world = np.dot(tf.transformations.translation_matrix(view_to_world[0]), tf.transformations.quaternion_matrix(view_to_world[1]))
    #print "view_to_world", view_to_world


    #print "zone_to_world", zone_to_world

    view_to_zone = np.dot(np.linalg.inv(zone_to_world), view_to_world)
    #print "view_to_zone", view_to_zone

    pp = points.copy()
    for i in range(3):
        points[:,:,i] = pp[:,:,0] * view_to_zone[i,0] + pp[:,:,1] * view_to_zone[i,1] + pp[:,:,2] * view_to_zone[i,2] + view_to_zone[i,3]

    pub_image_points.publish(bridge.cv2_to_imgmsg((points[:,:,1] * 256 + 127).astype(np.uint8), "passthrough"))

    boxtest = np.ones(d.shape)

    boxtest[d >= 1024] = 0
    boxtest[d <= 0] = 0

    #boxtest[points[:,:,2] < 0.9] = 0
    #boxtest[points[:,:,2] > 2.1] = 0

    #boxtest[points[:,:,0] < -1.0] = 0
    #boxtest[points[:,:,0] > 5.0] = 0

    boxtest[points[:,:,0] < -0.5] = 0
    boxtest[points[:,:,0] > +0.5] = 0

    boxtest[points[:,:,1] < -0.5] = 0
    boxtest[points[:,:,1] > +0.5] = 0

    boxtest[points[:,:,2] < -0.5] = 0
    boxtest[points[:,:,2] > +0.5] = 0

    points[boxtest == 0] = [0,0,0]
    d[boxtest == 0] = 0

    d[d <= 0] = 1024
    d = cv2.erode(d, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 25)), iterations=3)
    d[d >= 1024] = 0

    threshold = 0.3
    d[np.abs(d - cv2.dilate(d, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5)), iterations=1)) > threshold] = 0
    d[np.abs(d - cv2.erode(d, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5)), iterations=1)) > threshold] = 0

    #pub_image_points.publish(bridge.cv2_to_imgmsg((points * 50 + 127).astype(np.uint8), "passthrough"))
    #pub_image_points.publish(bridge.cv2_to_imgmsg((points[:,:,:] * 20 + 127).astype(np.uint8), "passthrough"))

    d2 = np.zeros(d.shape[:2], np.uint8)
    d2[:,:] = 255
    #d2[edges > 1.0] = 0
    d2[d <= 0.0] = 0

    mask = np.zeros((d.shape[0] + 2, d.shape[1] + 2), np.uint8)

    w = d2.shape[1] - 1
    h = d2.shape[0] - 1

    #if True:
    #    t = 10
    #    for f in np.arange(0, 1, 0.01):
    #        floodFill2(d2, mask, (int((w - t - t) * f + t), t), 0)
    #        floodFill2(d2, mask, (t, int((h - t - t) * f + t)), 0)
    #        floodFill2(d2, mask, (w - t, int((h - t - t) * f + t)), 0)

    #d2 = cv2.medianBlur(d2, 3)
    #d2 = cv2.dilate(d2, np.ones((3, 3),np.uint8), iterations = 3)

    faces = sorted(faces, key=lambda x: x[2] * x[3], reverse=True)

    faces2 = [ ]

    (clustercount, clusters, stats, centroids) = cv2.connectedComponentsWithStats(d2, 8, cv2.CV_32S)
    clusters2 = np.zeros(clusters.shape, np.uint8)
    clusters2[:,:] = clusters[:,:] * 100

    pub_image_clusters.publish(bridge.cv2_to_imgmsg(clusters2, "mono8"))

    for (x,y,w,h) in faces:
      if d2[y, x + w / 2] > 0 and d2[y, x + w / 2] != 100:
        z = d[y, x + w / 2]
        cv2.floodFill(d2, mask, (x + w / 2, y), 100, 50)
        faces2.append((x, y, w, h, z, ))

    #faces2 = faces

    image2 = image.copy()
    image2[d2 == 100,0] = 0
    image2[d2 == 100,1] = 255
    image2[d2 == 100,2] = 0

    image = image / 2 + image2 / 2

    image_d = d2
    image_d = cv2.cvtColor(image_d, cv2.COLOR_GRAY2BGR)

    image_c = image.copy()
    for (x,y,w,h) in faces:
      s = w / 2 + h / 2
      cv2.circle(image_c, (x + w / 2, y + h / 2), s / 2, (0,0,255), 4)
      cv2.circle(image_d, (x + w / 2, y + h / 2), s / 2, (0,0,255), 4)

    msg_person_detections = person_detection.msg.PersonDetections()

    markers = visualization_msgs.msg.MarkerArray()

    i = 0

    for (x,y,w,h,z) in faces2:


        #print(K)
        screen_to_view = np.linalg.inv(K)
        #print(screen_to_view)
        point_on_screen = np.array([x, y, 1.0], dtype=float)
        direction = np.dot(screen_to_view, point_on_screen)

        position = direction * z
        #print(position)

        direction = direction / np.linalg.norm(direction)
        #print(direction)

        x -= w / 2
        y -= h / 2
        y -= h / 5
        w += w / 1
        h += h / 1

        if x < 0 or y < 0 or x + w >= image.shape[1] or y + h >= image.shape[0]:
            continue

        m = cv2.resize(face_marker, (w, h))

        a = cv2.getRotationMatrix2D((m.shape[0]/2, m.shape[1]/2), rospy.Time.now().to_sec() * 100, 1)
        m = cv2.warpAffine(m, a, (m.shape[0], m.shape[1]))

        m = cv2.copyMakeBorder(m, y, image.shape[0] - y - h, x, image.shape[1] - x - w, cv2.BORDER_CONSTANT, (0,0,0,0))
        #print m.shape, image.shape
        #print image
        image = numpy.asarray(PIL.Image.alpha_composite(PIL.Image.fromarray(image), PIL.Image.fromarray(m)))



        msg_person_detection = person_detection.msg.PersonDetection()

        msg_person_detection.header.stamp = depth_image_header.stamp
        msg_person_detection.header.frame_id = depth_image_header.frame_id

        msg_person_detection.x = (x * 2.0 / image.shape[1] - 1.0) * image.shape[1] / image.shape[0]
        msg_person_detection.y = -(y * 2.0 / image.shape[0] - 1.0)
        msg_person_detection.size = w * 2.0 / image.shape[1]

        msg_person_detection.direction.x = direction[0]
        msg_person_detection.direction.y = direction[1]
        msg_person_detection.direction.z = direction[2]

        msg_person_detection.position.x = position[0]
        msg_person_detection.position.y = position[1]
        msg_person_detection.position.z = position[2]

        msg_person_detections.detections.append(msg_person_detection)


        marker = visualization_msgs.msg.Marker()

        #marker.header.stamp = rospy.Time.now()
        marker.header.frame_id = camera_depth_frame

        #marker.action = visualization_msgs.msg.Marker.ADD

        marker.ns = "persons"
        marker.id = i

        marker.lifetime.secs = 1.0

        marker.type = visualization_msgs.msg.Marker.SPHERE

        marker.color.r = 0.0
        marker.color.g = 1.0
        marker.color.b = 0.0
        marker.color.a = 1.0

        marker.scale.x = 0.3
        marker.scale.y = 0.3
        marker.scale.z = 0.3

        marker.pose.position.x = position[0]
        marker.pose.position.y = position[1]
        marker.pose.position.z = position[2]

        marker.pose.orientation.x = 0;
        marker.pose.orientation.y = 0;
        marker.pose.orientation.z = 0;
        marker.pose.orientation.w = 1;

        marker.frame_locked = False

        markers.markers.append(marker)

        i = i + 1




    marker = visualization_msgs.msg.Marker()

    marker.header.frame_id = depth_image_header.frame_id

    marker.ns = "cage"
    marker.id = 10000

    marker.lifetime.secs = 1.0

    '''
    marker.type = visualization_msgs.msg.Marker.CUBE

    mat = np.linalg.inv(view_to_zone)
    #print mat
    #mat = view_to_zone
    scale, shear, angles, trans, persp = tf.transformations.decompose_matrix(mat)
    mat[0,3] = 0
    mat[1,3] = 0
    mat[2,3] = 0
    mat[3,3] = 1
    mat[0:3,0] /= np.linalg.norm(mat[0:3,0])
    mat[0:3,1] /= np.linalg.norm(mat[0:3,1])
    mat[0:3,2] /= np.linalg.norm(mat[0:3,2])
    quat = tf.transformations.quaternion_from_matrix(mat)
    #quat = tf.transformations.quaternion_inverse(quat)

    marker.color.r = 0.0
    marker.color.g = 1.0
    marker.color.b = 1.0
    marker.color.a = 0.2

    marker.scale.x = scale[0]
    marker.scale.y = scale[1]
    marker.scale.z = scale[2]

    marker.pose.position.x = trans[0]
    marker.pose.position.y = trans[1]
    marker.pose.position.z = trans[2]

    marker.pose.orientation.x = quat[1]
    marker.pose.orientation.y = quat[2]
    marker.pose.orientation.z = quat[3]
    marker.pose.orientation.w = quat[0]
    '''

    marker.type = visualization_msgs.msg.Marker.LINE_LIST

    zone_to_view = np.linalg.inv(view_to_zone)

    vpoints = [
        [+0.5, +0.5, +0.5],
        [+0.5, +0.5, -0.5],
        [+0.5, -0.5, +0.5],
        [+0.5, -0.5, -0.5],
        [-0.5, +0.5, +0.5],
        [-0.5, +0.5, -0.5],
        [-0.5, -0.5, +0.5],
        [-0.5, -0.5, -0.5],
    ]

    vpoints = [p + [1.0] for p in vpoints]

    vpoints = [[[q, p] for q in vpoints if np.sum(np.array(q) == np.array(p)) == 3] for p in vpoints]
    #vpoints = [[[q, p] for q in vpoints] for p in vpoints]
    vpoints = np.reshape(vpoints, [-1, 4])
    #print vpoints

    for p in vpoints:
        p = np.dot(zone_to_view, p)
        q = geometry_msgs.msg.Point()
        q.x = p[0]
        q.y = p[1]
        q.z = p[2]
        marker.points.append(q)

    marker.color.r = 0.0
    marker.color.g = 1.0
    marker.color.b = 1.0
    marker.color.a = 0.2

    marker.scale.x = 0.01
    marker.scale.y = 1
    marker.scale.z = 1

    marker.pose.position.x = 0
    marker.pose.position.y = 0
    marker.pose.position.z = 0

    marker.pose.orientation.x = 0
    marker.pose.orientation.y = 0
    marker.pose.orientation.z = 0
    marker.pose.orientation.w = 1

    marker.frame_locked = False

    markers.markers.append(marker)


    pub_person_detections.publish(msg_person_detections)

    pub_markers.publish(markers)


    image_c = image_c * (1.0 / 255)
    image_c[:,:,1] = depth

    image = image[:,:,0:3]

    pub_image_visualization.publish(bridge.cv2_to_imgmsg(image, "bgr8"))

    #cv2.waitKey(1)






if True:

    markers = visualization_msgs.msg.MarkerArray()

    marker = visualization_msgs.msg.Marker()

    marker.header.frame_id = rospy.get_param("/tiago_bartender/customer_zone/frame")

    marker.ns = "cage_global"
    marker.id = 50000

    marker.type = visualization_msgs.msg.Marker.LINE_LIST

    vpoints = [
        [+0.5, +0.5, +0.5],
        [+0.5, +0.5, -0.5],
        [+0.5, -0.5, +0.5],
        [+0.5, -0.5, -0.5],
        [-0.5, +0.5, +0.5],
        [-0.5, +0.5, -0.5],
        [-0.5, -0.5, +0.5],
        [-0.5, -0.5, -0.5],
    ]

    vpoints = [p + [1.0] for p in vpoints]

    vpoints = [[[q, p] for q in vpoints if np.sum(np.array(q) == np.array(p)) == 3] for p in vpoints]
    vpoints = np.reshape(vpoints, [-1, 4])

    for p in vpoints:
        p = np.dot(zone_to_world, p)
        q = geometry_msgs.msg.Point()
        q.x = p[0]
        q.y = p[1]
        q.z = p[2]
        marker.points.append(q)

    marker.color.r = 1.0
    marker.color.g = 0.0
    marker.color.b = 1.0
    marker.color.a = 0.2

    marker.scale.x = 0.03
    marker.scale.y = 1
    marker.scale.z = 1

    marker.pose.position.x = 0
    marker.pose.position.y = 0
    marker.pose.position.z = 0

    marker.pose.orientation.x = 0
    marker.pose.orientation.y = 0
    marker.pose.orientation.z = 0
    marker.pose.orientation.w = 1

    marker.frame_locked = False

    markers.markers.append(marker)

    pub_markers.publish(markers)




while not rospy.is_shutdown():
  if enabled:
    update()
    rospy.sleep(0.01)
  else:
    rospy.sleep(0.2)
