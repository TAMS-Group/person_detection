#!/usr/bin/env python

import sys
import rospy
import cv2
from std_msgs.msg import String
from sensor_msgs.msg import Image
from cv_bridge import CvBridge, CvBridgeError
import PIL.Image
import numpy
import numpy as np
#import operator
import sensor_msgs
import std_msgs
import person_detection
import person_detection.msg
import sensor_msgs.msg
import tf
import visualization_msgs.msg

rospy.init_node("person_detection", anonymous=True)

#db = "haarcascade_frontalface_default.xml"
#db = "haarcascade_frontalface_alt.xml"
#db = "haarcascade_frontalcatface_extended.xml"
#db = "haarcascade_fullbody.xml"
#db = "haarcascade_frontalface_alt2.xml"
db = rospy.get_param("~face_detector_model")
face_cascade = cv2.CascadeClassifier(db)

bridge = CvBridge()

pub_image_visualization = rospy.Publisher("~visualization", sensor_msgs.msg.Image)
pub_image_clusters = rospy.Publisher("~clusters", sensor_msgs.msg.Image)

#face_marker = cv2.imread('smiley-1635449_960_720.png', cv2.IMREAD_UNCHANGED)
#print("a", rospy.search_param("face_marker"))
#print("b", rospy.get_param(rospy.search_param("face_marker")))
#face_marker = cv2.imread(rospy.get_param(rospy.search_param("face_marker")), cv2.IMREAD_UNCHANGED)

#tf_listener = tf.TransformListener()

face_marker = rospy.get_param("~face_marker")
print("b", face_marker)
face_marker = cv2.imread(face_marker, cv2.IMREAD_UNCHANGED)

face_marker[:,:,3] -= face_marker[:,:,3] / 3

depth = False

color_image = False

#cv2.namedWindow("image", cv2.WINDOW_NORMAL)

enabled = False

enabled = rospy.get_param("~enabled", False)

def enable(x):
    global enabled
    print("enable")
    enabled = True

def disable(x):
    global enabled
    print("disable")
    enabled = False

def set_enabled(x):
    global enabled
    x = x.data
    print("set_enabled", x)
    enabled = x

sub_enable = rospy.Subscriber("~enable", std_msgs.msg.Empty, enable)
sub_disable = rospy.Subscriber("~disable", std_msgs.msg.Empty, disable)
sub_enabled = rospy.Subscriber("~set_enabled", std_msgs.msg.Bool, set_enabled)

camera_depth_info = False
def on_camera_depth_info(msg):
    global camera_depth_info
    #print("bla")
    camera_depth_info = msg
sub_camera_depth_info = rospy.Subscriber("camera_depth_info", sensor_msgs.msg.CameraInfo, on_camera_depth_info)

camera_depth_frame = rospy.get_param("~camera_depth_frame")

pub_person_detections = rospy.Publisher("~person_detections", person_detection.msg.PersonDetections)

pub_markers = rospy.Publisher("~person_markers", visualization_msgs.msg.MarkerArray)

def image_callback(data):
    global bridge, face_cascade, depth, color_image

    try:
        image = bridge.imgmsg_to_cv2(data, "bgra8")
    except CvBridgeError as e:
        print(e)
        return

    color_image = image

def floodFill2(image, mask, point, value):
    if image[point[1], point[0]] > 0:
        cv2.floodFill(image, mask, point, value)

last_faces = False

def update():
    #print "update"

    global bridge, face_cascade, depth, color_image, last_faces

    if camera_depth_info is False:
        return

    image = color_image
    if image is False:
        print "no image"
        return
    image = image.copy()

    if False:
        image = cv2.resize(image, (0,0), fx=1.1, fy=1.1)

    y = (image.shape[1] - color_image.shape[1]) / 2
    x = (image.shape[0] - color_image.shape[0]) / 2
    w = color_image.shape[1]
    h = color_image.shape[0]
    image = image[y:y+h,x:x+w,:]
    #print x, y, w, h, image.shape

    #a = cv2.getRotationMatrix2D((m.shape[0]/2, m.shape[1]/2), rospy.Time.now().to_sec() * 300, 1)
    #image = cv2.warpAffine(image, a, (image.shape[0], image.shape[1]))


    if depth is False:
        print "no depth"
        return

    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    #cv2.imshow("gray", gray)
    #cv2.waitKey(1)

    faces = face_cascade.detectMultiScale(gray, 1.2, 1)

    if last_faces is not False and len(last_faces) > 0:

      #faces = [np.argmin(np.array(last_faces)[:,0:2] - np.array(face)[0:2]) for face in faces]

      for face in faces:
        best_match = last_faces[0]
        for face_prev in last_faces:
          if best_match is not False and np.linalg.norm(np.array(best_match)[0:2] - np.array(face)[0:2]) > np.linalg.norm(np.array(face_prev)[0:2] - np.array(face)[0:2]):
            best_match = face_prev
        if False:
            if best_match is not False:
              face[2] = face[2] * 0.2 + best_match[2] * 0.8
              face[3] = face[3] * 0.2 + best_match[3] * 0.8

    last_faces = faces

    d = depth.copy()

    #d[d > 0.0] = 0.0
    d[:,:] = 1024
    m = depth > 0.0
    d[m] = depth[m]

    d = cv2.medianBlur(d, 7)
    d = cv2.erode(d, np.ones((2,2),np.uint8), iterations = 15)

    d[d == 1024] = 0

    dx = cv2.Sobel(d, cv2.CV_32F, 1, 0, ksize = 7)
    dy = cv2.Sobel(d, cv2.CV_32F, 0, 1, ksize = 7)
    edges = cv2.max(cv2.max(dx, -dx), cv2.max(dy, -dy))
    edges = edges * 0.03

    ee = cv2.medianBlur(edges, 7)
    dx = cv2.Sobel(ee, cv2.CV_32F, 1, 0, ksize = 7)
    dy = cv2.Sobel(ee, cv2.CV_32F, 0, 1, ksize = 7)
    edges = cv2.max(cv2.max(dx, -dx), cv2.max(dy, -dy)) * 0.015

    #d[edges > 0.5] = 0.0

    #cv2.imshow("edges", edges)

    #edges = cv2.dilate(edges, np.ones((3,3),np.uint8), iterations = 7

    #d2 = d.copy()
    d2 = np.zeros(d.shape[:2], np.uint8)
    d2[:,:] = 255
    d2[edges > 1.0] = 0
    d2[d <= 0.0] = 0

    mask = np.zeros((d.shape[0] + 2, d.shape[1] + 2), np.uint8)


    w = d2.shape[1] - 1
    h = d2.shape[0] - 1

    if True:
        t = 10
        for f in np.arange(0, 1, 0.01):
            floodFill2(d2, mask, (int((w - t - t) * f + t), t), 0)
            #floodFill2(d2, mask, (int((w - d - d) * f + d), h - d), 0)
            floodFill2(d2, mask, (t, int((h - t - t) * f + t)), 0)
            floodFill2(d2, mask, (w - t, int((h - t - t) * f + t)), 0)

    d2 = cv2.medianBlur(d2, 3)
    d2 = cv2.dilate(d2, np.ones((3, 3),np.uint8), iterations = 3)

    faces = sorted(faces, key=lambda x: x[2] * x[3], reverse=True)

    faces2 = [ ]

    (clustercount, clusters, stats, centroids) = cv2.connectedComponentsWithStats(d2, 8, cv2.CV_32S)
    clusters2 = np.zeros(clusters.shape, np.uint8)
    clusters2[:,:] = clusters[:,:] * 100
    #cv2.imshow("clusters", clusters2)


    pub_image_clusters.publish(bridge.cv2_to_imgmsg(clusters2, "mono8"))

    #print(stats)

    for (x,y,w,h) in faces:
      #print("face", x, y, w, h)
      if d2[y, x + w / 2] > 0 and d2[y, x + w / 2] != 100:
        #cv2.floodFill(d2, mask, (x + w / 2, y + h / 2), 100, 50)
        z = d[y, x + w / 2]
        cv2.floodFill(d2, mask, (x + w / 2, y), 100, 50)
        faces2.append((x, y, w, h, z, ))



    #cv2.imshow("depth", d * 100.0)



    #cv2.imshow("depth2", d2)

    #d2 = cv2.erode(d2, np.ones((3,3),np.uint8), iterations = 15)

    image2 = image.copy()
    image2[d2 == 100,0] = 0
    image2[d2 == 100,1] = 255
    image2[d2 == 100,2] = 0

    image = image / 2 + image2 / 2


    image_d = d2
    image_d = cv2.cvtColor(image_d, cv2.COLOR_GRAY2BGR)

    image_c = image.copy()
    for (x,y,w,h) in faces:
      #cv2.ellipse(image_c, (x, y), (w, h), 0, 0, 0, (0,0,255), 4)
      s = w / 2 + h / 2
      cv2.circle(image_c, (x + w / 2, y + h / 2), s / 2, (0,0,255), 4)
      cv2.circle(image_d, (x + w / 2, y + h / 2), s / 2, (0,0,255), 4)

    #image_c[edges > 0.1,:] = 0;

    msg_person_detections = person_detection.msg.PersonDetections()

    markers = visualization_msgs.msg.MarkerArray()

    i = 0

    for (x,y,w,h,z) in faces2:

        #if d2[y, x] > 0:
        #cv2.rectangle(image, (x,y), (x+w,y+h), (255,0,0), 2)
        #m = cv2.resize(face_marker, (image.shape[1], image.shape[0]))

        #x -= w / 4
        #y -= h / 4
        #y -= h / 6
        #w += w / 2
        #h += h / 2

        K = np.array(camera_depth_info.K, dtype=float)
        K = np.reshape(K, (3,3,))
        #print(K)
        screen_to_view = np.linalg.inv(K)
        #print(screen_to_view)
        point_on_screen = np.array([x, y, 1.0], dtype=float)
        direction = np.dot(screen_to_view, point_on_screen)

        position = direction * z
        #print(position)

        direction = direction / np.linalg.norm(direction)
        #print(direction)

        x -= w / 2
        y -= h / 2
        y -= h / 5
        w += w / 1
        h += h / 1

        if x < 0 or y < 0 or x + w >= image.shape[1] or y + h >= image.shape[0]:
            continue

        m = cv2.resize(face_marker, (w, h))

        a = cv2.getRotationMatrix2D((m.shape[0]/2, m.shape[1]/2), rospy.Time.now().to_sec() * 100, 1)
        m = cv2.warpAffine(m, a, (m.shape[0], m.shape[1]))

        m = cv2.copyMakeBorder(m, y, image.shape[0] - y - h, x, image.shape[1] - x - w, cv2.BORDER_CONSTANT, (0,0,0,0))
        #print m.shape, image.shape
        image = numpy.asarray(PIL.Image.alpha_composite(PIL.Image.fromarray(image), PIL.Image.fromarray(m)))



        msg_person_detection = person_detection.msg.PersonDetection()

        msg_person_detection.x = (x * 2.0 / image.shape[1] - 1.0) * image.shape[1] / image.shape[0]
        msg_person_detection.y = -(y * 2.0 / image.shape[0] - 1.0)
        msg_person_detection.size = w * 2.0 / image.shape[1]

        msg_person_detection.direction.x = direction[0]
        msg_person_detection.direction.y = direction[1]
        msg_person_detection.direction.z = direction[2]

        msg_person_detection.position.x = position[0]
        msg_person_detection.position.y = position[1]
        msg_person_detection.position.z = position[2]

        msg_person_detections.detections.append(msg_person_detection)


        marker = visualization_msgs.msg.Marker()

        #marker.header.stamp = rospy.Time.now()
        marker.header.frame_id = camera_depth_frame

        #marker.action = visualization_msgs.msg.Marker.ADD

        marker.ns = "persons"
        marker.id = i

        marker.lifetime.secs = 1.0

        marker.type = visualization_msgs.msg.Marker.SPHERE

        marker.color.r = 0.0
        marker.color.g = 1.0
        marker.color.b = 0.0
        marker.color.a = 1.0

        marker.scale.x = 0.3
        marker.scale.y = 0.3
        marker.scale.z = 0.3

        marker.pose.position.x = position[0]
        marker.pose.position.y = position[1]
        marker.pose.position.z = position[2]

        marker.pose.orientation.x = 0;
        marker.pose.orientation.y = 0;
        marker.pose.orientation.z = 0;
        marker.pose.orientation.w = 1;

        marker.frame_locked = False

        markers.markers.append(marker)

        i = i + 1

    pub_person_detections.publish(msg_person_detections)

    pub_markers.publish(markers)


    image_c = image_c * (1.0 / 255)
    image_c[:,:,1] = depth

    #cv2.imshow("image_c", image_c)

    #cv2.imshow("image_d", image_d)

    #cv2.imshow("depth", depth)

    #cv2.imshow("image", image)

    #cv2.imshow("image", image)

    image = image[:,:,0:3]

    pub_image_visualization.publish(bridge.cv2_to_imgmsg(image, "bgr8"))

    cv2.waitKey(1)



#image_sub = rospy.Subscriber("/camera/rgb/image_rect_color", Image, image_callback)
#image_sub = rospy.Subscriber(rospy.get_param("camera_topic_color"), Image, image_callback)
image_sub = rospy.Subscriber("camera_rgb", Image, image_callback)

def depth_callback(data):
    global bridge, depth

    try:
        image = bridge.imgmsg_to_cv2(data)
    except CvBridgeError as e:
        print(e)
        return

    depth = image

    #cv2.imshow("depth", image)
    #cv2.waitKey(1)

#depth_sub = rospy.Subscriber("/camera/depth_registered/image_raw", Image, depth_callback)
#depth_sub = rospy.Subscriber(rospy.get_param("camera_topic_depth"), Image, depth_callback)
depth_sub = rospy.Subscriber("camera_depth", Image, depth_callback)

#rospy.spin()

while not rospy.is_shutdown():
  if enabled:
    update()
    rospy.sleep(0.01)
  else:
    rospy.sleep(0.2)
